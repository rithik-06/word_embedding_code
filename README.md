ğŸŒ Word Embedding from Scratch


This repository implements popular word embedding techniques from scratch using pure Python and NumPy, without relying on deep learning frameworks like PyTorch or TensorFlow.

Word embeddings are crucial in Natural Language Processing (NLP) for representing words in dense vector space where semantic similarity is preserved.

ğŸ“Œ Features
âœ¨ Word2Vec

Continuous Bag of Words (CBOW)

Skip-Gram model

ğŸ“‰ Negative Sampling and Softmax Loss

ğŸ”¡ Vocabulary building & one-hot encoding

ğŸ” Cosine similarity for word similarity comparison

ğŸ“Š Optional: PCA/t-SNE for 2D visualization of embeddings

ğŸ§  Algorithms Covered
CBOW (Continuous Bag of Words)
Predict the center word from surrounding context words.

Skip-Gram
Predict the context words given a center word.

Negative Sampling
Efficient training method by sampling negative examples.

ğŸ—‚ï¸ Project Structure
bash
Copy
Edit
word_embedding/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ corpus.txt                # Training corpus
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ cbow.py                   # CBOW implementation
â”‚   â””â”€â”€ skipgram.py               # Skip-Gram implementation
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ preprocessing.py          # Text cleaning, tokenization, vocab building
â”œâ”€â”€ train.py                      # Training loop and vector updates
â”œâ”€â”€ visualize.py                  # Optional: Plotting word vectors
â””â”€â”€ README.md
